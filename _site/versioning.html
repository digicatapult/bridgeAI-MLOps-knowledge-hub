<!DOCTYPE html>
<html lang="en-US">
<head>
<meta charset="UTF-8">

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>BridgeAI MLOps Knowledge Hub</title>
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="BridgeAI MLOps Knowledge Hub" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="http://localhost:4000/versioning.html" />
<meta property="og:url" content="http://localhost:4000/versioning.html" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="BridgeAI MLOps Knowledge Hub" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","headline":"BridgeAI MLOps Knowledge Hub","url":"http://localhost:4000/versioning.html"}</script>
<!-- End Jekyll SEO tag -->

<link rel="preconnect" href="https://fonts.gstatic.com">
<link rel="preload" href="https://fonts.googleapis.com/css?family=Open+Sans:400,700&display=swap" as="style" type="text/css" crossorigin>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="theme-color" content="#157878">
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
<link rel="stylesheet" href="/assets/css/style.css?v=">
<!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup Google Analytics -->



<!-- You can set your favicon here -->
<!-- link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" -->

<!-- end custom head snippets -->

</head>
<body>
<a id="skip-to-content" href="#content">Skip to the content.</a>

<header class="page-header" role="banner">
<h1 class="project-name">BridgeAI MLOps Knowledge Hub</h1>
<!-- <img src="https://miro.medium.com/v2/resize:fit:800/0*566yfiZWjA1k1WLN.png" alt="Knowledge Hub Cover"> -->
<!-- <br> -->


<style>
    body {
    font-family: "Lato", sans-serif;
    }
    
    .sidenav {
    height: 100%;
    width: 18%;
    position: fixed;
    top: 0;
    left: 0;
    background-color: rgb(24, 137, 105);
    overflow-x: hidden;
    transition: 0.5s;
    padding-top: 60px;
    }
    
    .sidenav a {
    text-align: left;
    padding: 8px 8px 8px 32px;
    text-decoration: none;
    font-size: 18px;
    color: #ffffff;
    display: block;
    transition: 0.3s;
    }
    
    .sidenav a:hover {
    color: #201d1d;
    }

    .dropdown {
    text-align: left;
    padding: 8px 8px 8px 32px;
    height: 0%;
    width: 100%;
    position: relative;
    display: inline-block;
    background-color: rgb(24, 137, 105);
    overflow-x: hidden;
    transition: 0.5s;
    }

    .dropdown a {
    text-align: left;
    font-size: 18px;
    color: #ffffff;
    transition: 0.3s;
    }
    
    .dropdown a:hover {
    color: #201d1d;
    }
    
    @media screen and (max-height: 450px) {
    .sidenav {padding-top: 15px;}
    .sidenav a {font-size: 18px;}
    }

    </style>
    <div id="mySidenav" class="sidenav">
    <a href="/">Home</a>
    <a href="/maturity_assessment.html">Maturity Assessment</a>
    <a onclick="toggleNav()">Team Journey</a>
    <div id="myDropdown" class="dropdown">
        <a href="/requirements_research.html">Requirements and Research</a>
        <a href="/mlflow_reg.html">MLflow Registry</a>
        <a href="/serving.html">Model Serving</a>
        <a href="/monitoring.html">Model Monitoring</a>
        <a href="/versioning.html">Data Versioning</a>
    </div>
    <a href="/prerequisites.html">Prerequisites - Skills, Roles and Tools</a>
    <a href="/effective_deployment.html">Effective Deployment - From PoC to Production</a>
    <a href="/best_practices.html">MLOps Best Practices</a>
</div>

<script>
function toggleNav() {
    const dropdown = document.getElementById("myDropdown");
    if (dropdown.style.height === "280px") {
        dropdown.style.height = "0px";
    } else {
        dropdown.style.height = "280px";
    }
}
</script>
</header>

<main id="content" class="main-content" role="main">
<h1 id="data-versioning-spike-content">Data Versioning Spike Content</h1>

<p><strong>Table of Contents</strong></p>
<ol>
  <li><a href="#1-general-comparison">General Comparison</a><br />
     <a href="#11-dvc">DVC</a><br />
     <a href="#12-mlflow">MLFlow</a><br />
     <a href="#13-general-comparison-table">General Comparison Table</a></li>
  <li><a href="#2-data-versioning">Data Versioning</a><br />
     <a href="#21-dvc">DVC</a><br />
     <a href="#22-mlflow">MLFlow</a></li>
  <li><a href="#3-where-the-data-is-stored">Where Data is Stored</a><br />
     <a href="#31-dvc">DVC</a><br />
     <a href="#32-mlflow">MLFlow</a></li>
  <li><a href="#4-how-to-version">How to Version</a>
     <a href="#41-dvc">DVC</a><br />
     <a href="#42-mlflow">MLFlow</a></li>
  <li><a href="#5-potential-options">Potential Options</a>
     <a href="#51-using-mlflow">Using MLFlow</a><br />
     <a href="#52-use-dvc-within-train-repo">Using DVC Within Train Repo</a><br />
     <a href="#53-use-dvc-for-data-versioning-during-ingestion-with-train-eval-and-test-split">Using DVC for Data Versioning During Ingestion with Train, Eval, and Test Split</a>
<a href="#summary">Summary</a><br />
<a href="#references">References</a></li>
</ol>

<blockquote>
  <p>Here is a summary of the investigation conducted to determine which version control tool is best suited for our MLOps pipeline and integrates well with our existing technology and tool stack.</p>

  <p>The tools compared are MLFlow and DVC (Data Version Control).</p>
</blockquote>

<p><img src="image-20240724-204151.png" />
An image showing the different versions of data, features and model. Source: <a href="https://dvc.org/doc/use-cases/versioning-data-and-model-files">DVC</a></p>

<hr />

<p>The type of data versioning each tool supports:</p>

<p>Standalone data versioning</p>

<p>Associating data with a training run in an experiment</p>

<p>Where the data is stored (remote, local, or database)</p>

<p>How to perform versioning with each tool (including sample steps and code snippets)</p>

<p>How to retrieve versioned data for re-training or reproducibility</p>

<p>A verdict on which tool is more suitable for our current use case</p>

<hr />

<h2 id="1-general-comparison">1. General Comparison</h2>
<h3 id="11-dvc">1.1 DVC</h3>
<p>DVC(Data Version Control) is an open-source version control system specifically designed to handle large datasets and machine learning models. It focuses on data versioning, reproducibility, and collaboration.</p>

<p>DVC provides a way to manage data and machine learning models in a version-controlled manner, similar to how Git handles code.</p>

<h3 id="12-mlflow">1.2 MLFlow</h3>
<p>MLFlow is an open-source platform designed to manage the end-to-end machine learning lifecycle, including experimentation, reproducibility, and deployment. It is particularly well-suited for tracking and managing machine learning experiments.</p>

<p>MLflow offers the ability to track datasets that are associated with model training events. These metadata associated with the Dataset can be stored through the use of the mlflow.log_input() API.</p>

<h3 id="13-general-comparison-table">1.3 General Comparison Table:</h3>

<table>
  <thead>
    <tr>
      <th>Feature</th>
      <th>MLFlow</th>
      <th>DVC</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Primary Focus</td>
      <td>Experiment tracking and model management</td>
      <td>Data versioning and pipeline management</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td>Data Versioning</td>
      <td>Limited, relies on external tools</td>
      <td>Comprehensive, designed for large datasets</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td>Experiment Tracking</td>
      <td>Yes, detailed tracking and comparison</td>
      <td>Basic, primarily through Git</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td>Pipeline Management</td>
      <td>Basic, experimental</td>
      <td>Advanced, tracks data and code changes</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td>Model Management</td>
      <td>Advanced, with deployment options</td>
      <td>Basic, through versioning</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td>Integration</td>
      <td>Integrates well with various ML tools</td>
      <td>Integrates with Git and various storage backends</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td>Reproducibility</td>
      <td>Focus on reproducibility of experiments</td>
      <td>Strong focus, tracks entire pipeline along with data and model</td>
    </tr>
  </tbody>
  <tbody>
    <tr>
      <td>Ease of Use</td>
      <td>User-friendly interface, extensive documentation, simple python API</td>
      <td>Requires understanding of Git, more command-line oriented</td>
    </tr>
  </tbody>
</table>

<h2 id="2-data-versioning">2. Data Versioning</h2>

<h3 id="21-dvc">2.1 DVC</h3>

<p>DVC tracks changes in data files and directories</p>

<p>When you add data files to DVC, it creates metadata files (.dvc files) that store information about the data, including its version and remote location</p>

<p>These DVC files can be committed to a Git repository, allowing you to track the data’s history and lineage along with your code.</p>

<h3 id="22-mlflow">2.2 MLFlow</h3>

<p>MLFlow doesn’t really can do version control of data independently, it relies on external tools</p>

<p>However, the <code class="language-plaintext highlighter-rouge">mlflow.data</code> module helps you record your model training and evaluation datasets to runs and stores in artefact store</p>

<p>It can later retrieve dataset information from runs</p>

<h2 id="3-where-the-data-is-stored">3. Where the Data is Stored</h2>

<h3 id="31-dvc">3.1 DVC</h3>

<p>DVC remotes are distributed storage locations for the data sets and ML models. It is similar to Git remotes, but for cached assets. This optional feature is typically used to share or back up copies of all or some of your data. Several types are supported: Amazon S3, Google Drive, SSH, HTTP, local file systems, among others.</p>

<p><a href="https://dvc.org/doc/command-reference/remote#remote">DVC remote</a></p>

<h3 id="32-mlflow">3.2 MLFlow</h3>
<p>MLFlow tracking needs the following components.</p>

<p><strong>Backend store</strong>
Experiment metadata including run ID, start &amp; end time, parameters, metrics, etc are stored in Backend store. MLflow supports two types of storage for the backend: file-system-based like local files and database-based like PostgreSQL. Default is sqlite.</p>

<p><strong>Artefact store</strong></p>

<p>Artifact store persists typically large artifacts for each run, such as model weights (e.g. a pickled scikit-learn model, pytorch model, etc) and data files (e.g. csv or parquet files). MLflow stores artifacts in a local file (mlruns) by default, but also supports different storage options such as Amazon S3 and Azure Blob Storage.</p>

<p>Backend store - Backend Stores — MLflow 2.15.1 documentation</p>

<p>Artefact store - Artifact Stores — MLflow 2.15.1 documentation</p>

<p>Tracking server - MLflow Tracking Server</p>

<h2 id="4-how-to-version">4. How to Version</h2>

<h3 id="41-dvc">4.1 DVC</h3>

<p>DVC uses git and similar to git commands to version data.</p>

<p>Install DVC: <code class="language-plaintext highlighter-rouge">pip install dvc</code></p>

<p>Initialize DVC from a git repo: <code class="language-plaintext highlighter-rouge">dvc init</code></p>

<p>Configure remote (s3): <code class="language-plaintext highlighter-rouge">dvc remote add -d myremote s3://mybucket/path/to/dvcstore</code></p>

<p>Add data files or directories: <code class="language-plaintext highlighter-rouge">dvc add training_data test_data data.csv</code></p>

<p>Commit changes: <code class="language-plaintext highlighter-rouge">git add *.dvc .gitignore</code> and <code class="language-plaintext highlighter-rouge">git commit -m "Add data files to DVC"</code></p>

<p>Push data to the remote: <code class="language-plaintext highlighter-rouge">dvc push</code></p>

<p>Tag repo for the data version [optional]: <code class="language-plaintext highlighter-rouge">git tag -a v1.0 -m "Version 1.0"</code> and <code class="language-plaintext highlighter-rouge">git push origin v1.0</code></p>

<p>When pulling data from the repository: <code class="language-plaintext highlighter-rouge">dvc pull</code> or <code class="language-plaintext highlighter-rouge">dvc pull &lt;file&gt;.dvc</code></p>

<blockquote>
  <p>The above can be done in the same repo where the training is happening. This will make the data tightly coupled to the training. But if we do this outside the training repo, it will make the data easily retrievable from anywhere else for other use like getting the test result or for measuring the data drift.</p>
</blockquote>

<h3 id="42-mlflow">4.2 MLFlow</h3>

<p>As mentioned earlier, MLFlow doesn’t really version data, but helps to track the data associated with each experiment run.</p>

<p>MLFlow uses the following major interfaces for data tracking.</p>

<p>Dataset: Represents a dataset used in model training or evaluation, including features, targets, predictions, and metadata such as the dataset’s name, digest (hash) schema, profile, and source.</p>

<p>You can log this metadata to a run in MLflow Tracking using the mlflow.log_input() API. mlflow.data provides APIs for constructing Datasets from a variety of Python data objects, including Pandas DataFrames (<code class="language-plaintext highlighter-rouge">mlflow.data.from_pandas()</code>), NumPy arrays (<code class="language-plaintext highlighter-rouge">mlflow.data.from_numpy()</code>), Spark DataFrames (<code class="language-plaintext highlighter-rouge">mlflow.data.from_spark()</code> / <code class="language-plaintext highlighter-rouge">mlflow.data.load_delta()</code>) and more.</p>

<p>DatasetSource: Represents the source of a dataset. For example, this may be a directory of files stored in S3, a Delta Table, or a web URL. Each Dataset references the source from which it was derived. A Dataset’s features and targets may differ from the source if transformations and filtering were applied.</p>

<p>You can get the DatasetSource of a dataset logged to a run in MLflow Tracking using the <code class="language-plaintext highlighter-rouge">mlflow.data.get_source()</code> API.</p>

<p><strong>Detailed steps:</strong></p>

<p>Create a dataset</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import mlflow.data
import pandas as pd
from mlflow.data.pandas_dataset import PandasDataset
dataset_source_url = "http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv"
df = pd.read_csv(dataset_source_url)
# Construct an MLflow PandasDataset from the Pandas DataFrame, 
and specify the web URL as the source
dataset: PandasDataset = mlflow.data.from_pandas(df, source=dataset_source_url)
</code></pre></div></div>

<p>Log the dataset</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>with mlflow.start_run():
    # Log the dataset to the MLflow Run. Specify the "training" context 
    # to indicate that the dataset is used for model training
    mlflow.log_input(dataset, context="training")
</code></pre></div></div>

<p>Retrieve the dataset when needed</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Retrieve the run, including dataset information
run = mlflow.get_run(mlflow.last_active_run().info.run_id)
dataset_info = run.inputs.dataset_inputs[0].dataset
# Load the dataset's source, which downloads the content from the source URL
# to the local filesystem
dataset_source = mlflow.data.get_source(dataset_info)
dataset_source.load()
</code></pre></div></div>

<blockquote>
  <p>The above way of tracking the data is tightly coupled to the training. If the data (say test data) needs to be used outside the training repo, it can be done by using the run id from MLFlow.</p>
</blockquote>

<blockquote>
  <p>An alternative to using MLFlow.data might be using MLFlow’s log artefact to log the data as such to every run. But this will not track the difference, instead it just pushes the entire data used for the run to the artefact store.</p>
</blockquote>

<h2 id="5-potential-options">5. Potential Options</h2>
<h3 id="51-using-mlflow">5.1 Using MLFlow</h3>
<p>Use MLFlow to just log the data used for each experiment runs. Need artefact store set up(which anyway is needed for storing the models)</p>

<p><strong>Pros:</strong></p>

<p>No additional tool or library is needed</p>

<p><strong>Cons:</strong></p>

<p>Data needs to be converted to MLFlow Datasets using simple steps</p>

<p>Not really versioning, it is tracking the data used for training runs</p>

<p>Extracting the data form outside the train repo is difficult. It needs the exact MLFlow run_id and hence adds additional dependencies</p>

<h3 id="52-use-dvc-within-train-repo">5.2 Use dvc within train repo</h3>
<p>This method can be used to track the data used for experiment runs.</p>

<p>Add the data directories or files to the dvc, and add the dvc metadata files to git from within the training repo. The train, validation and test split needs to be tracked here.</p>

<p><strong>Pros:</strong></p>

<p>No overhead of creating dataset. Data or directory can be versioned or tracked as it is.</p>

<p>Seamless integration with git</p>

<p><strong>Cons:</strong></p>

<p>Overhead of creating additional remote data storage for dvc</p>

<p>If the dvc is used within the same training repo, it makes a dependency on the entire training repo for retrieving the data outside the context of training (say for testing the accuracy or for model drift detection)</p>

<p>Introducing a different model in a different repo will introduce inconsistency in data split and versions.</p>

<h3 id="53-use-dvc-for-data-versioning-during-ingestion-with-train-eval-and-test-split">5.3 Use dvc for data versioning during ingestion with train, eval, and test split</h3>
<p>With this approach the workflow is as follows;</p>

<p>→ Use dvc in the data ingestion repo (with version tagging to the repo to indicate the data version)</p>

<p>→  Do ETL related preprocessing of the data as usual in the repo</p>

<p>→ Split the data into train, val and test and add those data separately to the dvc</p>

<p>→ Push these dvc files (metadata) to a remote</p>

<p>→ Git commit, tag and push the meta data files</p>

<p>→ Clone or use the data ingestion repo as a submodule in the model training repo or wherever data is needed</p>

<p>→ Checkout to the data version that is needed</p>

<p>→ Pull the data from the remote to local</p>

<p>→ Train as usual</p>

<p><strong>Pros:</strong></p>

<p>Single source of truth for train, test and validation data</p>

<p>Data is decoupled and can be retrieved using just version from anywhere from the data ingestion repo</p>

<p>The dataset associated with experiment runs will be captured in MLFlow parameters along with other params</p>

<p><strong>Cons:</strong></p>

<p>Overhead of creating additional remote data storage for dvc</p>

<p>Overhead of cloning the data ingestion repo or using it as a submodule wherever data is needed</p>

<h2 id="summary">Summary</h2>
<p>As per the comparison of the potential options, based on our use case, the third option seems suitable for our use case.</p>

<h2 id="references">References</h2>
<p>DVC official documentation - Home</p>

<p>MLFlow data official documentation - mlflow.data</p>

<p>DVC remote -remote</p>

<p>MLFlow Backend store - Backend Stores — MLflow 2.15.1 documentation</p>

<p>MLFlow Artefact store - Artifact Stores — MLflow 2.15.1 documentation</p>

<p>MLFlow Tracking server - MLflow Tracking Server</p>

<footer class="site-footer">

<span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>
</footer>
</main>
</body>
</html>